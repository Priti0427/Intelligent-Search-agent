{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Search - Evaluation Notebook\n",
    "\n",
    "This notebook evaluates the Agentic Search system using IR metrics.\n",
    "\n",
    "## INFO 624: Intelligent Search and Language Models\n",
    "\n",
    "### Evaluation Metrics (Week 8 Concepts):\n",
    "- Precision@k\n",
    "- Recall@k\n",
    "- Mean Reciprocal Rank (MRR)\n",
    "- Answer Quality (RAGAS-style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "\n",
    "import asyncio\n",
    "import time\n",
    "from typing import List, Dict\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluation Metrics Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(relevant: List[int], retrieved: List[int], k: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Precision@k.\n",
    "    \n",
    "    Precision@k = (# relevant in top k) / k\n",
    "    \"\"\"\n",
    "    if k == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    top_k = retrieved[:k]\n",
    "    relevant_in_k = len(set(top_k) & set(relevant))\n",
    "    return relevant_in_k / k\n",
    "\n",
    "\n",
    "def recall_at_k(relevant: List[int], retrieved: List[int], k: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Recall@k.\n",
    "    \n",
    "    Recall@k = (# relevant in top k) / (# total relevant)\n",
    "    \"\"\"\n",
    "    if len(relevant) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    top_k = retrieved[:k]\n",
    "    relevant_in_k = len(set(top_k) & set(relevant))\n",
    "    return relevant_in_k / len(relevant)\n",
    "\n",
    "\n",
    "def mean_reciprocal_rank(relevant: List[int], retrieved: List[int]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Mean Reciprocal Rank.\n",
    "    \n",
    "    MRR = 1 / (rank of first relevant result)\n",
    "    \"\"\"\n",
    "    for i, doc_id in enumerate(retrieved):\n",
    "        if doc_id in relevant:\n",
    "            return 1.0 / (i + 1)\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def average_precision(relevant: List[int], retrieved: List[int]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Average Precision.\n",
    "    \n",
    "    AP = sum(P@k * rel(k)) / |relevant|\n",
    "    \"\"\"\n",
    "    if len(relevant) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    score = 0.0\n",
    "    num_relevant = 0\n",
    "    \n",
    "    for i, doc_id in enumerate(retrieved):\n",
    "        if doc_id in relevant:\n",
    "            num_relevant += 1\n",
    "            score += num_relevant / (i + 1)\n",
    "    \n",
    "    return score / len(relevant)\n",
    "\n",
    "\n",
    "# Test the metrics\n",
    "relevant = [1, 3, 5, 7]\n",
    "retrieved = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "print(f\"Precision@5: {precision_at_k(relevant, retrieved, 5):.3f}\")\n",
    "print(f\"Recall@5: {recall_at_k(relevant, retrieved, 5):.3f}\")\n",
    "print(f\"MRR: {mean_reciprocal_rank(relevant, retrieved):.3f}\")\n",
    "print(f\"AP: {average_precision(relevant, retrieved):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Answer Quality Evaluation\n",
    "\n",
    "Implementing RAGAS-style metrics for answer quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from src.utils.config import get_settings\n",
    "\n",
    "settings = get_settings()\n",
    "\n",
    "async def evaluate_faithfulness(answer: str, context: str) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate if the answer is faithful to the context.\n",
    "    \n",
    "    Faithfulness = claims in answer supported by context / total claims\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(\n",
    "        model=settings.openai_model,\n",
    "        api_key=settings.openai_api_key,\n",
    "        temperature=0,\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"Evaluate if the following answer is faithful to the given context.\n",
    "    \n",
    "Context:\n",
    "{context[:2000]}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Score the faithfulness from 0 to 1, where:\n",
    "- 1.0 = All claims in the answer are supported by the context\n",
    "- 0.5 = Some claims are supported, some are not\n",
    "- 0.0 = No claims are supported by the context\n",
    "\n",
    "Respond with just a number between 0 and 1.\"\"\"\n",
    "    \n",
    "    response = await llm.ainvoke(prompt)\n",
    "    try:\n",
    "        return float(response.content.strip())\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "\n",
    "async def evaluate_relevance(answer: str, query: str) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate if the answer is relevant to the query.\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(\n",
    "        model=settings.openai_model,\n",
    "        api_key=settings.openai_api_key,\n",
    "        temperature=0,\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"Evaluate if the following answer is relevant to the query.\n",
    "    \n",
    "Query: {query}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Score the relevance from 0 to 1, where:\n",
    "- 1.0 = The answer directly and completely addresses the query\n",
    "- 0.5 = The answer partially addresses the query\n",
    "- 0.0 = The answer is not relevant to the query\n",
    "\n",
    "Respond with just a number between 0 and 1.\"\"\"\n",
    "    \n",
    "    response = await llm.ainvoke(prompt)\n",
    "    try:\n",
    "        return float(response.content.strip())\n",
    "    except:\n",
    "        return 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Benchmark Test Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test queries with expected characteristics\n",
    "test_queries = [\n",
    "    {\n",
    "        \"query\": \"What is RAG in AI?\",\n",
    "        \"type\": \"simple\",\n",
    "        \"expected_sources\": [\"web\", \"academic\"],\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Compare BM25 and dense retrieval for question answering\",\n",
    "        \"type\": \"complex\",\n",
    "        \"expected_sources\": [\"academic\", \"web\"],\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How does BERT improve search ranking compared to TF-IDF, and what are the computational tradeoffs?\",\n",
    "        \"type\": \"multi_hop\",\n",
    "        \"expected_sources\": [\"academic\", \"web\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(test_queries)} test queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agent import run_search\n",
    "\n",
    "async def run_evaluation(queries: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Run evaluation on test queries.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for q in queries:\n",
    "        print(f\"\\nEvaluating: {q['query'][:50]}...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        result = await run_search(q['query'])\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        # Extract metrics\n",
    "        answer = result.get('final_answer', result.get('draft_answer', ''))\n",
    "        quality = result.get('overall_quality', 0)\n",
    "        iterations = result.get('iteration_count', 0)\n",
    "        \n",
    "        # Count results by source\n",
    "        web_count = len(result.get('web_results', []))\n",
    "        vector_count = len(result.get('vector_results', []))\n",
    "        arxiv_count = len(result.get('arxiv_results', []))\n",
    "        \n",
    "        results.append({\n",
    "            'query': q['query'],\n",
    "            'expected_type': q['type'],\n",
    "            'actual_type': result.get('query_type', 'unknown'),\n",
    "            'quality_score': quality,\n",
    "            'iterations': iterations,\n",
    "            'elapsed_seconds': elapsed,\n",
    "            'web_results': web_count,\n",
    "            'vector_results': vector_count,\n",
    "            'arxiv_results': arxiv_count,\n",
    "            'answer_length': len(answer),\n",
    "        })\n",
    "        \n",
    "        print(f\"  Type: {result.get('query_type')} | Quality: {quality:.2f} | Time: {elapsed:.1f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run evaluation (uncomment to execute)\n",
    "# eval_results = await run_evaluation(test_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample results for demonstration\n",
    "sample_results = [\n",
    "    {'query': 'What is RAG?', 'quality_score': 0.85, 'elapsed_seconds': 5.2, 'iterations': 1},\n",
    "    {'query': 'Compare BM25 and dense retrieval', 'quality_score': 0.78, 'elapsed_seconds': 8.1, 'iterations': 2},\n",
    "    {'query': 'BERT vs TF-IDF tradeoffs', 'quality_score': 0.72, 'elapsed_seconds': 12.3, 'iterations': 2},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(sample_results)\n",
    "print(\"Evaluation Summary:\")\n",
    "print(f\"  Average Quality: {df['quality_score'].mean():.2f}\")\n",
    "print(f\"  Average Time: {df['elapsed_seconds'].mean():.1f}s\")\n",
    "print(f\"  Average Iterations: {df['iterations'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison: Classical vs Neural Retrieval\n",
    "\n",
    "Demonstrating course concepts by comparing retrieval methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual comparison table\n",
    "comparison = {\n",
    "    'Method': ['TF-IDF', 'BM25', 'Dense (BERT)', 'Hybrid'],\n",
    "    'Type': ['Sparse', 'Sparse', 'Dense', 'Both'],\n",
    "    'Semantic': ['No', 'No', 'Yes', 'Yes'],\n",
    "    'Speed': ['Fast', 'Fast', 'Slower', 'Medium'],\n",
    "    'Week': ['4', '6', '5', '11'],\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison)\n",
    "print(\"\\nRetrieval Methods Comparison (Course Alignment):\")\n",
    "print(df_comparison.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
